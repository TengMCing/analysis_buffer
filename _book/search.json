[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":" place record temporary data analysis ideas.","code":""},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"use-the-entire-dataset-to-perform-power-analysis","chapter":"1 Use the entire dataset to perform power analysis","heading":"1 Use the entire dataset to perform power analysis","text":"idea use conventional tests decide model correctly specified regards type departures. may lead Type III error though.","code":""},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"load-libraries","chapter":"1 Use the entire dataset to perform power analysis","heading":"1.1 Load libraries","text":"","code":"\nlibrary(tidyverse)\nlibrary(visage)"},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"load-data","chapter":"1 Use the entire dataset to perform power analysis","heading":"1.2 Load data","text":"","code":""},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"visual-test-data","chapter":"1 Use the entire dataset to perform power analysis","heading":"1.2.1 Visual test data","text":"","code":"\nvisual_test_dat <- vi_survey %>%\n  filter(x_dist == \"uniform\", !attention_check, !null_lineup) %>%\n  select(unique_lineup_id, effect_size, type, p_value) %>%\n  group_by(unique_lineup_id) %>%\n  summarise(across(everything(), first))\nhead(visual_test_dat)\n#> # A tibble: 6 × 4\n#>   unique_lineup_id effect_size type                p_value\n#>   <chr>                  <dbl> <chr>                 <dbl>\n#> 1 heter_101             272.   heteroskedasticity 0.000269\n#> 2 heter_105               1.84 heteroskedasticity 0.824   \n#> 3 heter_110              20.6  heteroskedasticity 0.924   \n#> 4 heter_116              40.8  heteroskedasticity 0.0438  \n#> 5 heter_120               7.97 heteroskedasticity 0.0147  \n#> 6 heter_121              44.3  heteroskedasticity 0.0605"},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"conventional-test-data","chapter":"1 Use the entire dataset to perform power analysis","heading":"1.2.2 Conventional test data","text":"","code":"\npoly_conv_sim <- readRDS(\"data/poly_conventional_simulation.rds\")\nheter_conv_sim <- readRDS(\"data/heter_conventional_simulation.rds\")\n\n# Borrow effect size from the survey\npoly_conv_sim <- poly_conv_sim %>%\n  left_join(select(filter(vi_survey, type == \"polynomial\"), \n                   shape, e_sigma, n, x_dist, effect_size))\n\nheter_conv_sim <- heter_conv_sim %>%\n  left_join(select(filter(vi_survey, type == \"heteroskedasticity\"), \n                   a, b, n, x_dist, effect_size))\nconv_test_dat <- bind_rows(poly_conv_sim, heter_conv_sim) %>%\n  filter(x_dist == \"uniform\")\nhead(conv_test_dat)\n#> # A tibble: 6 × 19\n#>   shape e_sigma x_dist     n F_p_v…¹ RESET…² RESET…³ RESET…⁴\n#>   <dbl>   <dbl> <chr>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1     3       1 unifo…    50 5.24e-5   0.152 9.15e-4 8.29e-4\n#> 2     3       1 unifo…    50 5.24e-5   0.152 9.15e-4 8.29e-4\n#> 3     3       1 unifo…    50 5.24e-5   0.152 9.15e-4 8.29e-4\n#> 4     3       1 unifo…    50 5.24e-5   0.152 9.15e-4 8.29e-4\n#> 5     3       1 unifo…    50 5.24e-5   0.152 9.15e-4 8.29e-4\n#> 6     3       1 unifo…    50 5.24e-5   0.152 9.15e-4 8.29e-4\n#> # … with 11 more variables: RESET6_p_value <dbl>,\n#> #   RESET7_p_value <dbl>, RESET8_p_value <dbl>,\n#> #   RESET9_p_value <dbl>, RESET10_p_value <dbl>,\n#> #   BP_p_value <dbl>, SW_p_value <dbl>, boot_id <int>,\n#> #   effect_size <dbl>, a <dbl>, b <dbl>, and abbreviated\n#> #   variable names ¹​F_p_value, ²​RESET3_p_value,\n#> #   ³​RESET4_p_value, ⁴​RESET5_p_value"},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"compute-glm-for-visual-test","chapter":"1 Use the entire dataset to perform power analysis","heading":"1.3 Compute GLM for visual test","text":"","code":""},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"define-the-minimum-and-maximum-effect-size","chapter":"1 Use the entire dataset to perform power analysis","heading":"1.3.1 Define the minimum and maximum effect size","text":"","code":"\nmin_es <- vi_survey %>% \n  filter(!null_lineup, \n         !attention_check,\n         x_dist == \"uniform\") %>%\n  pull(effect_size) %>%\n  min()\n\nmax_es <- vi_survey %>% \n  filter(!null_lineup, \n         !attention_check,\n         x_dist == \"uniform\") %>%\n  pull(effect_size) %>%\n  max()\nc(log(min_es), log(max_es))\n#> [1] -1.450875  6.132414"},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"fit-the-model","chapter":"1 Use the entire dataset to perform power analysis","heading":"1.3.2 Fit the model","text":"","code":"\nvisual_mod <- visual_test_dat %>%\n  mutate(offset0 = log(0.05/0.95)) %>%\n  mutate(reject = p_value <= 0.05) %>%\n  \n  # Slope-only model\n  glm(reject ~ effect_size - 1,\n      family = binomial(),\n      data = .,\n      offset = offset0)\nsummary(visual_mod)\n#> \n#> Call:\n#> glm(formula = reject ~ effect_size - 1, family = binomial(), \n#>     data = ., offset = offset0)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -3.0104  -0.4070  -0.3307   0.1368   2.2719  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> effect_size 0.098306   0.007402   13.28   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 741.18  on 279  degrees of freedom\n#> Residual deviance: 195.65  on 278  degrees of freedom\n#> AIC: 197.65\n#> \n#> Number of Fisher Scoring iterations: 7"},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"make-prediction-for-the-visual-model","chapter":"1 Use the entire dataset to perform power analysis","heading":"1.3.3 Make prediction for the visual model","text":"","code":"\nvisual_pred <- data.frame(effect_size = exp(seq(log(min_es), \n                                                log(max_es), \n                                                0.1))) %>%\n  mutate(power = predict(visual_mod, \n                         type = \"response\",\n                         newdata = data.frame(effect_size = effect_size,\n                                              offset0 = log(0.05/0.95)))) %>%\n  mutate(log_effect_size = log(effect_size))\nhead(visual_pred)\n#>   effect_size      power log_effect_size\n#> 1   0.2343650 0.05110579      -1.4508755\n#> 2   0.2590134 0.05122343      -1.3508755\n#> 3   0.2862541 0.05135373      -1.2508755\n#> 4   0.3163597 0.05149810      -1.1508755\n#> 5   0.3496315 0.05165810      -1.0508755\n#> 6   0.3864026 0.05183548      -0.9508755"},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"make-bootstrap-prediction-for-the-visual-model","chapter":"1 Use the entire dataset to perform power analysis","heading":"1.3.4 Make bootstrap prediction for the visual model","text":"","code":"\nvisual_boot_pred <- map_dfr(1:500, function(boot_id) {\n  \n  boot_mod <- visual_test_dat %>%\n    mutate(offset0 = log(0.05/0.95)) %>%\n    mutate(reject = p_value <= 0.05) %>%\n    slice_sample(n = nrow(.), replace = TRUE) %>%\n    update(visual_mod, data = .)\n  \n  data.frame(effect_size = exp(seq(log(min_es), \n                                   log(max_es), \n                                   0.1))) %>%\n  mutate(power = predict(boot_mod, \n                         type = \"response\",\n                         newdata = data.frame(effect_size = effect_size,\n                                              offset0 = log(0.05/0.95)))) %>%\n  mutate(log_effect_size = log(effect_size)) %>%\n  mutate(boot_id = boot_id)\n})"},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"compute-glm-for-conventional-tests","chapter":"1 Use the entire dataset to perform power analysis","heading":"1.4 Compute GLM for conventional tests","text":"","code":"\nconv_pred <- conv_test_dat %>%\n  select(-RESET3_p_value, -(RESET5_p_value:RESET10_p_value), -F_p_value) %>%\n  rename(RESET_p_value = RESET4_p_value) %>%\n  pivot_longer(RESET_p_value:SW_p_value) %>%\n  mutate(name = gsub(\"_p_value\", \" test\", name)) %>%\n  mutate(reject = value <= 0.05) %>%\n  select(effect_size, name, reject) %>%\n  mutate(offset0 = log(0.05/0.95)) %>%\n  nest(dat = c(effect_size, offset0, reject)) %>%\n  mutate(mod = map(dat, \n                   ~glm(reject ~ effect_size - 1, \n                        family = binomial(), \n                        data = .x,\n                        offset = offset0))) %>%\n  mutate(power = map(mod, function(mod) {\n    data.frame(effect_size = exp(seq(log(min_es),\n                                     log(max_es),\n                                     0.1)),\n               offset0 = log(0.05/0.95)) %>%\n      mutate(power = predict(mod, type = \"response\", newdata = .))\n  })) %>%\n  select(-dat, -mod) %>%\n  unnest(power) %>%\n  select(-offset0) %>%\n  mutate(log_effect_size = log(effect_size))"},{"path":"use-the-entire-dataset-to-perform-power-analysis.html","id":"draw-the-plot","chapter":"1 Use the entire dataset to perform power analysis","heading":"1.5 Draw the plot","text":"","code":"\nggplot() +\n  geom_point(data = visual_test_dat,\n             aes(log(effect_size), as.numeric(p_value <= 0.05)),\n             alpha = 0.15) +\n  \n  geom_line(data = conv_pred,\n            aes(log_effect_size, power, col = name),\n            size = 1) +\n  \n  geom_line(data = visual_boot_pred,\n            aes(log_effect_size, power, col = \"visual test\", group = boot_id),\n            size = 1,\n            alpha = 0.01) +\n  \n  geom_line(data = visual_pred,\n            aes(log_effect_size, power, col = \"visual test\"), \n            size = 1) +\n  \n  theme_light() +\n  scale_color_manual(values = rev(rcartocolor::carto_pal(4, \"Vivid\"))) +\n  xlab(expression(log[e] (Effect_size))) +\n  ylab(\"Power\") +\n  labs(col = \"\", size = \"# lineups\")"},{"path":"conventional-group-tests.html","id":"conventional-group-tests","chapter":"2 Conventional group tests","heading":"2 Conventional group tests","text":"group certain conventional tests together single test, need adjust significance level individual test.Considering group test consisting RESET test BP test, can plot \\(p\\)-values based simulated data.can pick rectangle area bottom left corner \\(P(\\text{RESET rejects } H_0 \\text{ BP rejects } H_0) = \\alpha\\). side length rectangle denoted \\(\\alpha^*\\).","code":"\nset.seed(10086)\n\nstand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1\n\nconv_null_dat <- map_dfr(1:100000, function(i) {\n  \n  x_dist <- sample(c(\"uniform\", \n                     \"normal\", \n                     \"lognormal\", \n                     \"even_discrete\"), 1)\n  x <- switch(x_dist,\n              uniform = rand_uniform(-1, 1),\n              normal = {\n                raw_x <- rand_normal(sigma = 0.3)\n                closed_form(~stand_dist(raw_x))\n                },\n              lognormal = {\n                raw_x <- rand_lognormal(sigma = 0.6)\n                closed_form(~stand_dist(raw_x/3 - 1))\n                },\n              even_discrete = rand_uniform_d(k = 5, even = TRUE))\n        \n  mod <- poly_model(include_z = FALSE, x = x)\n  \n  n <- sample(c(50, 100, 300), 1)\n  \n  tmp_dat <- mod$gen(n)\n  \n  tibble(x_dist = x_dist,\n         n = n,\n         F_p_value = mod$test(tmp_dat)$p_value,\n         RESET_p_value = mod$test(tmp_dat, \n                                  test = \"RESET\", \n                                  power = 2:4, \n                                  power_type = \"fitted\")$p_value,\n         BP_p_value = HETER_MODEL$test(tmp_dat)$p_value,\n         SW_p_value = shapiro.test(tmp_dat$.resid)$p.value,\n         boot_id = i)\n})\nconv_null_dat %>%\n  ggplot() +\n  geom_density2d_filled(aes(RESET_p_value, BP_p_value)) +\n  scale_x_sqrt() +\n  scale_x_sqrt() +\n  xlab(\"RESET p-value\") +\n  ylab(\"BP p-value\")\nalpha_star <- map_dbl(seq(0.0001, 0.05, 0.0005), function(alpha_star) {\n  conv_null_dat %>%\n    filter(RESET_p_value <= alpha_star | BP_p_value <= alpha_star) %>%\n    nrow()/nrow(conv_null_dat)\n}) %>%\n  {seq(0.0001, 0.05, 0.0005)[which.min(abs(. - 0.05))]}\nalpha_star\n#> [1] 0.0271"},{"path":"conventional-group-tests.html","id":"fit-glm","chapter":"2 Conventional group tests","heading":"2.1 Fit GLM","text":"","code":"\nconv_test_group_dat <- conv_test_dat %>%\n  mutate(`Group test (RESET & BP)_p_value` = ifelse(RESET4_p_value <= alpha_star | BP_p_value <= alpha_star, 0, 1)) %>%\n  relocate(`Group test (RESET & BP)_p_value`, .after = SW_p_value)\nconv_pred_group <- conv_test_group_dat %>%\n  select(-RESET3_p_value, -(RESET5_p_value:RESET10_p_value), -F_p_value) %>%\n  rename(RESET_p_value = RESET4_p_value) %>%\n  pivot_longer(RESET_p_value:`Group test (RESET & BP)_p_value`) %>%\n  mutate(name = gsub(\"_p_value\", \" test\", name)) %>%\n  mutate(reject = value <= 0.05) %>%\n  select(effect_size, name, reject) %>%\n  mutate(offset0 = log(0.05/0.95)) %>%\n  nest(dat = c(effect_size, offset0, reject)) %>%\n  mutate(mod = map(dat, \n                   ~glm(reject ~ effect_size - 1, \n                        family = binomial(), \n                        data = .x,\n                        offset = offset0))) %>%\n  mutate(power = map(mod, function(mod) {\n    data.frame(effect_size = exp(seq(log(min_es),\n                                     log(max_es),\n                                     0.1)),\n               offset0 = log(0.05/0.95)) %>%\n      mutate(power = predict(mod, type = \"response\", newdata = .))\n  })) %>%\n  select(-dat, -mod) %>%\n  unnest(power) %>%\n  select(-offset0) %>%\n  mutate(log_effect_size = log(effect_size))"},{"path":"conventional-group-tests.html","id":"draw-plot","chapter":"2 Conventional group tests","heading":"2.2 Draw plot","text":"group test higher power visual test case.","code":"\nggplot() +\n  geom_point(data = visual_test_dat,\n             aes(log(effect_size), as.numeric(p_value <= 0.05)),\n             alpha = 0.15) +\n  \n  geom_line(data = conv_pred_group,\n            aes(log_effect_size, power, col = name),\n            size = 1) +\n  \n  geom_line(data = visual_boot_pred,\n            aes(log_effect_size, power, col = \"visual test\", group = boot_id),\n            size = 1,\n            alpha = 0.01) +\n  \n  geom_line(data = visual_pred,\n            aes(log_effect_size, power, col = \"visual test\"), \n            size = 1) +\n  \n  theme_light() +\n  scale_color_manual(values = rev(rcartocolor::carto_pal(5, \"Vivid\"))) +\n  xlab(expression(log[e] (Effect_size))) +\n  ylab(\"Power\") +\n  labs(col = \"\", size = \"# lineups\")"},{"path":"group-test-reset-bp-and-sw.html","id":"group-test-reset-bp-and-sw","chapter":"3 Group test (RESET, BP and SW)","heading":"3 Group test (RESET, BP and SW)","text":"Let’s consider group test consisting RESET test, BP test SW test.","code":"\nalpha_star2 <- map_dbl(seq(0.0001, 0.05, 0.0005), function(alpha_star) {\n  conv_null_dat %>%\n    filter(RESET_p_value <= alpha_star | SW_p_value <= alpha_star) %>%\n    nrow()/nrow(conv_null_dat)\n}) %>%\n  {seq(0.0001, 0.05, 0.0005)[which.min(abs(. - 0.05))]}\nalpha_star2\n#> [1] 0.0246\nalpha_star3 <- map_dbl(seq(0.0001, 0.05, 0.0005), function(alpha_star) {\n  conv_null_dat %>%\n    filter(BP_p_value <= alpha_star | SW_p_value <= alpha_star) %>%\n    nrow()/nrow(conv_null_dat)\n}) %>%\n  {seq(0.0001, 0.05, 0.0005)[which.min(abs(. - 0.05))]}\nalpha_star3\n#> [1] 0.0266\nalpha_star4 <- map_dbl(seq(0.0001, 0.05, 0.0005), function(alpha_star) {\n  conv_null_dat %>%\n    filter(RESET_p_value <= alpha_star | BP_p_value <= alpha_star | SW_p_value <= alpha_star) %>%\n    nrow()/nrow(conv_null_dat)\n}) %>%\n  {seq(0.0001, 0.05, 0.0005)[which.min(abs(. - 0.05))]}\nalpha_star4\n#> [1] 0.0176"},{"path":"group-test-reset-bp-and-sw.html","id":"fit-glm-1","chapter":"3 Group test (RESET, BP and SW)","heading":"3.1 Fit GLM","text":"","code":"\nconv_test_group_dat <- conv_test_dat %>%\n  mutate(`Group (RESET & BP)_p_value` = ifelse(RESET4_p_value <= alpha_star | BP_p_value <= alpha_star, 0, 1)) %>%\n  relocate(`Group (RESET & BP)_p_value`, .after = SW_p_value) %>%\n  \n  mutate(`Group (RESET & SW)_p_value` = ifelse(RESET4_p_value <= alpha_star2 | SW_p_value <= alpha_star2, 0, 1)) %>%\n  relocate(`Group (RESET & SW)_p_value`, .after = `Group (RESET & BP)_p_value`) %>%\n  \n  mutate(`Group (BP & SW)_p_value` = ifelse(BP_p_value <= alpha_star3 | SW_p_value <= alpha_star3, 0, 1)) %>%\n  relocate(`Group (BP & SW)_p_value`, .after = `Group (RESET & SW)_p_value`) %>%\n  \n  mutate(`Group (RESET & BP & SW)_p_value` = ifelse(RESET4_p_value <= alpha_star3 | BP_p_value <= alpha_star3 | SW_p_value <= alpha_star3, 0, 1)) %>%\n  relocate(`Group (RESET & BP & SW)_p_value`, .after = `Group (BP & SW)_p_value`)\nconv_pred_group <- conv_test_group_dat %>%\n  select(-RESET3_p_value, -(RESET5_p_value:RESET10_p_value), -F_p_value) %>%\n  rename(RESET_p_value = RESET4_p_value) %>%\n  pivot_longer(RESET_p_value:`Group (RESET & BP & SW)_p_value`) %>%\n  mutate(name = gsub(\"_p_value\", \" test\", name)) %>%\n  mutate(reject = value <= 0.05) %>%\n  select(effect_size, name, reject) %>%\n  mutate(offset0 = log(0.05/0.95)) %>%\n  nest(dat = c(effect_size, offset0, reject)) %>%\n  mutate(mod = map(dat, \n                   ~glm(reject ~ effect_size - 1, \n                        family = binomial(), \n                        data = .x,\n                        offset = offset0))) %>%\n  mutate(power = map(mod, function(mod) {\n    data.frame(effect_size = exp(seq(log(min_es),\n                                     log(max_es),\n                                     0.1)),\n               offset0 = log(0.05/0.95)) %>%\n      mutate(power = predict(mod, type = \"response\", newdata = .))\n  })) %>%\n  select(-dat, -mod) %>%\n  unnest(power) %>%\n  select(-offset0) %>%\n  mutate(log_effect_size = log(effect_size))\nconv_pred_group <- conv_pred_group %>%\n  mutate(name = factor(name, levels = c(\"Group (RESET & BP & SW) test\",\n                                        \"Group (RESET & BP) test\",\n                                        \"Visual test\",\n                                        \"Group (RESET & SW) test\",\n                                        \"Group (BP & SW) test\",\n                                        \"BP test\",\n                                        \"SW test\",\n                                        \"RESET test\")))"},{"path":"group-test-reset-bp-and-sw.html","id":"draw-plot-1","chapter":"3 Group test (RESET, BP and SW)","heading":"3.2 Draw plot","text":"sensitivity tests ordered follows:Group test (RESET & BP & SW)Group test (RESET & BP)Visual testGroup test (RESET & SW) testGroup test (BP & SW) testBP testSW testRESET test","code":"\nggplot() +\n  geom_point(data = visual_test_dat,\n             aes(log(effect_size), as.numeric(p_value <= 0.05)),\n             alpha = 0.15) +\n  \n  geom_line(data = conv_pred_group,\n            aes(log_effect_size, power, col = name),\n            size = 1) +\n  \n  geom_line(data = visual_boot_pred,\n            aes(log_effect_size, power, col = \"Visual test\", group = boot_id),\n            size = 1,\n            alpha = 0.01) +\n\n  geom_line(data = visual_pred,\n            aes(log_effect_size, power, col = \"Visual test\"),\n            size = 1) +\n  \n  theme_light() +\n  # scale_color_brewer(palette = \"Set3\") +\n  scale_color_manual(breaks = levels(conv_pred_group$name), \n                     values = rev(rcartocolor::carto_pal(8, \"Vivid\"))) +\n  xlab(expression(log[e] (Effect_size))) +\n  ylab(\"Power\") +\n  labs(col = \"\", size = \"# lineups\") +\n  theme(legend.position = \"bottom\")"},{"path":"compute-vision-models.html","id":"compute-vision-models","chapter":"4 Compute vision models","heading":"4 Compute vision models","text":"","code":""},{"path":"compute-vision-models.html","id":"data","chapter":"4 Compute vision models","heading":"4.1 Data","text":"8860 observations vi_survey. includes attention checks null lineups. need data, cost per observation around 0.25 Australian Dollar. time needs collecting data around 6 minutes per observation.","code":""},{"path":"compute-vision-models.html","id":"types-of-model-grouped-by-types-of-training-data","chapter":"4 Compute vision models","heading":"4.2 Types of model grouped by types of training data","text":"Pure human training dataSimulated data + human dataPure simulated dataIf want model can mimic behaviour human, direct way train use human data.larger training set can formed simulated data also used.Model trained pure simulated data may perform like humans. like conventional test summarizes information residuals. Though may like visual test conducted human, still valuable.","code":""},{"path":"compute-vision-models.html","id":"concerns","chapter":"4 Compute vision models","heading":"4.2.0.1 Concerns","text":"sufficient data train model? say generally need >1000 images label.Can model detect residual departures non-linearity heteroskedasticity?\n- depends. model actually learns select different residual plot, chance detecting departures. model learns detect shapes feed, probably able , unless departures share similar characteristics non-linearity heteroskedasticity.Can model detect non-linearity heteroskedasticity shapes design?","code":""},{"path":"compute-vision-models.html","id":"potential-methods","chapter":"4 Compute vision models","heading":"4.3 Potential methods","text":"https://paperswithcode.com/","code":""},{"path":"compute-vision-models.html","id":"cores-of-computer-vision-models","chapter":"4 Compute vision models","heading":"4.3.1 Cores of computer vision models","text":"CNN\nPooling layers\nConvolutional layers\nPooling layersConvolutional layersBoltzmann family\nRestricted Boltzmann Machines\nDeep Belief Networks\nDeep Boltzmann Machines\nRestricted Boltzmann MachinesDeep Belief NetworksDeep Boltzmann MachinesAutoencoder\nDenoising Autoencoders\nStacked Autoencoders\nDenoising AutoencodersStacked AutoencodersAutoencoders\nDistance two data points embedding space (e.g. Faiss - Facebook)\nAnomaly detection\nDistance two data points embedding space (e.g. Faiss - Facebook)Anomaly detection","code":""},{"path":"check-consistency-of-c_ik.html","id":"check-consistency-of-c_ik","chapter":"5 Check consistency of \\(c_i/K\\)","heading":"5 Check consistency of \\(c_i/K\\)","text":"\\[\\text{E}[c_i/K] = \\sum_{=1}^{m}\\frac{1}{m}p(\\text{detect}|s_j = )p(s_j = ).\\]\\(c_i/K\\) converge \\(\\text{E}[c_i/K]\\) \\(K \\\\infty\\).Check p-values given \\(c_i/K\\) different \\(K\\).Given average weighted detect \\(c_i/K\\), can determine p-value visual test. simple model (Binomial), possible care \\(\\text{#detect}/K\\).","code":"\nvi_survey %>%\n  filter(!attention_check) %>%\n  arrange(unique_lineup_id, weighted_detect) %>%\n  group_by(unique_lineup_id) %>%\n  summarise(total_k = n(), \n            ci = cumsum(weighted_detect), \n            k = 1:n(), \n            ci_div_k = cumsum(weighted_detect)/(1:n())) %>%\n  ggplot() +\n  geom_line(aes(k, ci_div_k, group = unique_lineup_id)) +\n  facet_wrap(~total_k, scales = \"free_x\")\nget_p_value <- function(n_eval, average_total_detect, alpha = 0.2) {\n  weighted_total_detect <- n_eval * average_total_detect\n  target_dist <- exact_dist(n_eval, 20, dist = \"dirichlet\")\n  floor_total <- floor(weighted_total_detect)\n  ceil_total <- ceiling(weighted_total_detect)\n  return(unname(sum(target_dist[(ceil_total:n_eval) + 1]) + \n                  (ceil_total - weighted_total_detect) * \n                  target_dist[floor_total + 1]))\n}\n\nggplot() +\n  geom_point(aes(5:100, map_dbl(5:100, ~get_p_value(.x, 0.2)))) +\n  scale_y_log10()"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
