# Use the entire dataset to perform power analysis

The idea is to use conventional tests to decide if the model is correctly specified in regards of the type of the departures. This may lead to Type III error though.

## Load libraries

```{r}
library(tidyverse)
library(visage)
```

## Load data

### Visual test data

```{r}
visual_test_dat <- vi_survey %>%
  filter(x_dist == "uniform", !attention_check, !null_lineup) %>%
  select(unique_lineup_id, effect_size, type, p_value) %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first))
```

```{r}
head(visual_test_dat)
```


### Conventional test data

```{r}
poly_conv_sim <- readRDS("data/poly_conventional_simulation.rds")
heter_conv_sim <- readRDS("data/heter_conventional_simulation.rds")

# Borrow effect size from the survey
poly_conv_sim <- poly_conv_sim %>%
  left_join(select(filter(vi_survey, type == "polynomial"), 
                   shape, e_sigma, n, x_dist, effect_size))

heter_conv_sim <- heter_conv_sim %>%
  left_join(select(filter(vi_survey, type == "heteroskedasticity"), 
                   a, b, n, x_dist, effect_size))
```


```{r}
conv_test_dat <- bind_rows(poly_conv_sim, heter_conv_sim) %>%
  filter(x_dist == "uniform")
head(conv_test_dat)
```

## Compute GLM for visual test

### Define the minimum and maximum effect size

```{r}
min_es <- vi_survey %>% 
  filter(!null_lineup, 
         !attention_check,
         x_dist == "uniform") %>%
  pull(effect_size) %>%
  min()

max_es <- vi_survey %>% 
  filter(!null_lineup, 
         !attention_check,
         x_dist == "uniform") %>%
  pull(effect_size) %>%
  max()
```

```{r}
c(log(min_es), log(max_es))
```


### Fit the model

```{r}
visual_mod <- visual_test_dat %>%
  mutate(offset0 = log(0.05/0.95)) %>%
  mutate(reject = p_value <= 0.05) %>%
  
  # Slope-only model
  glm(reject ~ effect_size - 1,
      family = binomial(),
      data = .,
      offset = offset0)
```

```{r}
summary(visual_mod)
```


### Make prediction for the visual model

```{r}
visual_pred <- data.frame(effect_size = exp(seq(log(min_es), 
                                                log(max_es), 
                                                0.1))) %>%
  mutate(power = predict(visual_mod, 
                         type = "response",
                         newdata = data.frame(effect_size = effect_size,
                                              offset0 = log(0.05/0.95)))) %>%
  mutate(log_effect_size = log(effect_size))
```

```{r}
head(visual_pred)
```

### Make bootstrap prediction for the visual model

```{r cache = TRUE}
visual_boot_pred <- map_dfr(1:500, function(boot_id) {
  
  boot_mod <- visual_test_dat %>%
    mutate(offset0 = log(0.05/0.95)) %>%
    mutate(reject = p_value <= 0.05) %>%
    slice_sample(n = nrow(.), replace = TRUE) %>%
    update(visual_mod, data = .)
  
  data.frame(effect_size = exp(seq(log(min_es), 
                                   log(max_es), 
                                   0.1))) %>%
  mutate(power = predict(boot_mod, 
                         type = "response",
                         newdata = data.frame(effect_size = effect_size,
                                              offset0 = log(0.05/0.95)))) %>%
  mutate(log_effect_size = log(effect_size)) %>%
  mutate(boot_id = boot_id)
})

```


## Compute GLM for conventional tests

```{r cache = TRUE}
conv_pred <- conv_test_dat %>%
  select(-RESET3_p_value, -(RESET5_p_value:RESET10_p_value), -F_p_value) %>%
  rename(RESET_p_value = RESET4_p_value) %>%
  pivot_longer(RESET_p_value:SW_p_value) %>%
  mutate(name = gsub("_p_value", " test", name)) %>%
  mutate(reject = value <= 0.05) %>%
  select(effect_size, name, reject) %>%
  mutate(offset0 = log(0.05/0.95)) %>%
  nest(dat = c(effect_size, offset0, reject)) %>%
  mutate(mod = map(dat, 
                   ~glm(reject ~ effect_size - 1, 
                        family = binomial(), 
                        data = .x,
                        offset = offset0))) %>%
  mutate(power = map(mod, function(mod) {
    data.frame(effect_size = exp(seq(log(min_es),
                                     log(max_es),
                                     0.1)),
               offset0 = log(0.05/0.95)) %>%
      mutate(power = predict(mod, type = "response", newdata = .))
  })) %>%
  select(-dat, -mod) %>%
  unnest(power) %>%
  select(-offset0) %>%
  mutate(log_effect_size = log(effect_size))
```

## Draw the plot

```{r}
ggplot() +
  geom_point(data = visual_test_dat,
             aes(log(effect_size), as.numeric(p_value <= 0.05)),
             alpha = 0.15) +
  
  geom_line(data = conv_pred,
            aes(log_effect_size, power, col = name),
            size = 1) +
  
  geom_line(data = visual_boot_pred,
            aes(log_effect_size, power, col = "visual test", group = boot_id),
            size = 1,
            alpha = 0.01) +
  
  geom_line(data = visual_pred,
            aes(log_effect_size, power, col = "visual test"), 
            size = 1) +
  
  theme_light() +
  scale_color_manual(values = rev(rcartocolor::carto_pal(4, "Vivid"))) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  labs(col = "", size = "# lineups")
```


# Conventional group tests

If we group certain conventional tests together as a single test, we need to adjust the significance level for each individual test.

Considering a group test consisting of a RESET test and a BP test, we can plot their $p$-values against each other based on the simulated data.

```{r cache = TRUE}
set.seed(10086)

stand_dist <- function(x) (x - min(x))/max(x - min(x)) * 2 - 1

conv_null_dat <- map_dfr(1:100000, function(i) {
  
  x_dist <- sample(c("uniform", 
                     "normal", 
                     "lognormal", 
                     "even_discrete"), 1)
  x <- switch(x_dist,
              uniform = rand_uniform(-1, 1),
              normal = {
                raw_x <- rand_normal(sigma = 0.3)
                closed_form(~stand_dist(raw_x))
                },
              lognormal = {
                raw_x <- rand_lognormal(sigma = 0.6)
                closed_form(~stand_dist(raw_x/3 - 1))
                },
              even_discrete = rand_uniform_d(k = 5, even = TRUE))
        
  mod <- poly_model(include_z = FALSE, x = x)
  
  n <- sample(c(50, 100, 300), 1)
  
  tmp_dat <- mod$gen(n)
  
  tibble(x_dist = x_dist,
         n = n,
         F_p_value = mod$test(tmp_dat)$p_value,
         RESET_p_value = mod$test(tmp_dat, 
                                  test = "RESET", 
                                  power = 2:4, 
                                  power_type = "fitted")$p_value,
         BP_p_value = HETER_MODEL$test(tmp_dat)$p_value,
         SW_p_value = shapiro.test(tmp_dat$.resid)$p.value,
         boot_id = i)
})
```


```{r}
conv_null_dat %>%
  ggplot() +
  geom_density2d_filled(aes(RESET_p_value, BP_p_value)) +
  scale_x_sqrt() +
  scale_x_sqrt() +
  xlab("RESET p-value") +
  ylab("BP p-value")
```


We can pick a rectangle area at the bottom left corner such that $P(\text{RESET rejects } H_0 \text{ or BP rejects } H_0) = \alpha$. The side length of this rectangle is denoted as $\alpha^*$.

```{r}
alpha_star <- map_dbl(seq(0.0001, 0.05, 0.0005), function(alpha_star) {
  conv_null_dat %>%
    filter(RESET_p_value <= alpha_star | BP_p_value <= alpha_star) %>%
    nrow()/nrow(conv_null_dat)
}) %>%
  {seq(0.0001, 0.05, 0.0005)[which.min(abs(. - 0.05))]}
```

```{r}
alpha_star
```

## Fit GLM

```{r}
conv_test_group_dat <- conv_test_dat %>%
  mutate(`Group test (RESET & BP)_p_value` = ifelse(RESET4_p_value <= alpha_star | BP_p_value <= alpha_star, 0, 1)) %>%
  relocate(`Group test (RESET & BP)_p_value`, .after = SW_p_value)
```

```{r cache = TRUE}
conv_pred_group <- conv_test_group_dat %>%
  select(-RESET3_p_value, -(RESET5_p_value:RESET10_p_value), -F_p_value) %>%
  rename(RESET_p_value = RESET4_p_value) %>%
  pivot_longer(RESET_p_value:`Group test (RESET & BP)_p_value`) %>%
  mutate(name = gsub("_p_value", " test", name)) %>%
  mutate(reject = value <= 0.05) %>%
  select(effect_size, name, reject) %>%
  mutate(offset0 = log(0.05/0.95)) %>%
  nest(dat = c(effect_size, offset0, reject)) %>%
  mutate(mod = map(dat, 
                   ~glm(reject ~ effect_size - 1, 
                        family = binomial(), 
                        data = .x,
                        offset = offset0))) %>%
  mutate(power = map(mod, function(mod) {
    data.frame(effect_size = exp(seq(log(min_es),
                                     log(max_es),
                                     0.1)),
               offset0 = log(0.05/0.95)) %>%
      mutate(power = predict(mod, type = "response", newdata = .))
  })) %>%
  select(-dat, -mod) %>%
  unnest(power) %>%
  select(-offset0) %>%
  mutate(log_effect_size = log(effect_size))
```

## Draw plot

```{r}
ggplot() +
  geom_point(data = visual_test_dat,
             aes(log(effect_size), as.numeric(p_value <= 0.05)),
             alpha = 0.15) +
  
  geom_line(data = conv_pred_group,
            aes(log_effect_size, power, col = name),
            size = 1) +
  
  geom_line(data = visual_boot_pred,
            aes(log_effect_size, power, col = "visual test", group = boot_id),
            size = 1,
            alpha = 0.01) +
  
  geom_line(data = visual_pred,
            aes(log_effect_size, power, col = "visual test"), 
            size = 1) +
  
  theme_light() +
  scale_color_manual(values = rev(rcartocolor::carto_pal(5, "Vivid"))) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  labs(col = "", size = "# lineups")
```

The group test has higher power than the visual test in this case.

# Group test (RESET, BP and SW)

Let's consider a group test consisting of the RESET test, BP test and the SW test.

```{r}
alpha_star2 <- map_dbl(seq(0.0001, 0.05, 0.0005), function(alpha_star) {
  conv_null_dat %>%
    filter(RESET_p_value <= alpha_star | SW_p_value <= alpha_star) %>%
    nrow()/nrow(conv_null_dat)
}) %>%
  {seq(0.0001, 0.05, 0.0005)[which.min(abs(. - 0.05))]}
```

```{r}
alpha_star2
```


```{r}
alpha_star3 <- map_dbl(seq(0.0001, 0.05, 0.0005), function(alpha_star) {
  conv_null_dat %>%
    filter(BP_p_value <= alpha_star | SW_p_value <= alpha_star) %>%
    nrow()/nrow(conv_null_dat)
}) %>%
  {seq(0.0001, 0.05, 0.0005)[which.min(abs(. - 0.05))]}
```

```{r}
alpha_star3
```

```{r}
alpha_star4 <- map_dbl(seq(0.0001, 0.05, 0.0005), function(alpha_star) {
  conv_null_dat %>%
    filter(RESET_p_value <= alpha_star | BP_p_value <= alpha_star | SW_p_value <= alpha_star) %>%
    nrow()/nrow(conv_null_dat)
}) %>%
  {seq(0.0001, 0.05, 0.0005)[which.min(abs(. - 0.05))]}
```

```{r}
alpha_star4
```

## Fit GLM

```{r}
conv_test_group_dat <- conv_test_dat %>%
  mutate(`Group (RESET & BP)_p_value` = ifelse(RESET4_p_value <= alpha_star | BP_p_value <= alpha_star, 0, 1)) %>%
  relocate(`Group (RESET & BP)_p_value`, .after = SW_p_value) %>%
  
  mutate(`Group (RESET & SW)_p_value` = ifelse(RESET4_p_value <= alpha_star2 | SW_p_value <= alpha_star2, 0, 1)) %>%
  relocate(`Group (RESET & SW)_p_value`, .after = `Group (RESET & BP)_p_value`) %>%
  
  mutate(`Group (BP & SW)_p_value` = ifelse(BP_p_value <= alpha_star3 | SW_p_value <= alpha_star3, 0, 1)) %>%
  relocate(`Group (BP & SW)_p_value`, .after = `Group (RESET & SW)_p_value`) %>%
  
  mutate(`Group (RESET & BP & SW)_p_value` = ifelse(RESET4_p_value <= alpha_star3 | BP_p_value <= alpha_star3 | SW_p_value <= alpha_star3, 0, 1)) %>%
  relocate(`Group (RESET & BP & SW)_p_value`, .after = `Group (BP & SW)_p_value`)
```


```{r cache = TRUE}
conv_pred_group <- conv_test_group_dat %>%
  select(-RESET3_p_value, -(RESET5_p_value:RESET10_p_value), -F_p_value) %>%
  rename(RESET_p_value = RESET4_p_value) %>%
  pivot_longer(RESET_p_value:`Group (RESET & BP & SW)_p_value`) %>%
  mutate(name = gsub("_p_value", " test", name)) %>%
  mutate(reject = value <= 0.05) %>%
  select(effect_size, name, reject) %>%
  mutate(offset0 = log(0.05/0.95)) %>%
  nest(dat = c(effect_size, offset0, reject)) %>%
  mutate(mod = map(dat, 
                   ~glm(reject ~ effect_size - 1, 
                        family = binomial(), 
                        data = .x,
                        offset = offset0))) %>%
  mutate(power = map(mod, function(mod) {
    data.frame(effect_size = exp(seq(log(min_es),
                                     log(max_es),
                                     0.1)),
               offset0 = log(0.05/0.95)) %>%
      mutate(power = predict(mod, type = "response", newdata = .))
  })) %>%
  select(-dat, -mod) %>%
  unnest(power) %>%
  select(-offset0) %>%
  mutate(log_effect_size = log(effect_size))
```

```{r}
conv_pred_group <- conv_pred_group %>%
  mutate(name = factor(name, levels = c("Group (RESET & BP & SW) test",
                                        "Group (RESET & BP) test",
                                        "Visual test",
                                        "Group (RESET & SW) test",
                                        "Group (BP & SW) test",
                                        "BP test",
                                        "SW test",
                                        "RESET test")))
```


## Draw plot

```{r}
ggplot() +
  geom_point(data = visual_test_dat,
             aes(log(effect_size), as.numeric(p_value <= 0.05)),
             alpha = 0.15) +
  
  geom_line(data = conv_pred_group,
            aes(log_effect_size, power, col = name),
            size = 1) +
  
  geom_line(data = visual_boot_pred,
            aes(log_effect_size, power, col = "Visual test", group = boot_id),
            size = 1,
            alpha = 0.01) +

  geom_line(data = visual_pred,
            aes(log_effect_size, power, col = "Visual test"),
            size = 1) +
  
  theme_light() +
  # scale_color_brewer(palette = "Set3") +
  scale_color_manual(breaks = levels(conv_pred_group$name), 
                     values = rev(rcartocolor::carto_pal(8, "Vivid"))) +
  xlab(expression(log[e] (Effect_size))) +
  ylab("Power") +
  labs(col = "", size = "# lineups") +
  theme(legend.position = "bottom")
```


The sensitivity of the tests are ordered as follows: 

- Group test (RESET & BP & SW)
- Group test (RESET & BP)
- Visual test
- Group test (RESET & SW) test
- Group test (BP & SW) test
- BP test
- SW test
- RESET test



# Compute vision models

## Data

There are `r nrow(visage::vi_survey)` observations in `vi_survey`. This includes all the attention checks and null lineups. If we need more data, the cost per observation is around 0.25 Australian Dollar. The time needs for collecting the data is around 6 minutes per observation. 

## Types of model grouped by types of training data

- Pure human training data
- Simulated data + human data
- Pure simulated data

If we want a model that can mimic the behaviour of human, the direct way to train it is to use human data. 

A larger training set can be formed if simulated data are also used.

Model trained with pure simulated data may not perform like humans. It is more like a conventional test that summarizes the information of residuals. Though it may not like a visual test conducted by human, it is still valuable.

#### Concerns

Do we have sufficient data to train such a model? Some say we generally need >1000 images for a label.

Can the model detect residual departures other than non-linearity and heteroskedasticity? 
- It depends. If the model actually learns to select the most different residual plot, there is a chance of detecting other departures. But if the model only learns to detect the shapes we feed, then it probably will not be able to do it, unless other departures share similar characteristics of non-linearity and heteroskedasticity.

Can the model detect non-linearity and heteroskedasticity shapes other than what we design?


## Potential methods

https://paperswithcode.com/

### Cores of computer vision models

- CNN
  - Pooling layers
  - Convolutional layers
- Boltzmann family
  - Restricted Boltzmann Machines
  - Deep Belief Networks
  - Deep Boltzmann Machines
- Autoencoder
  - Denoising Autoencoders
  - Stacked Autoencoders



- Autoencoders
  - Distance between two data points in the embedding space (e.g. Faiss - Facebook)
  - Anomaly detection
  
# What we want from the automatic visual inference system?

It seems like we are unclear about the input and output of the system.

## Single residual plot - p-value

Consider a system take a single residual plot as input, and output a value between 0 and 1 indicating the probability of the null hypothesis is incorrect. Such a system clearly does not strictly follow the lineup protocol. It will try to estimate the result of the visual test without simulating the lineup protocol. Whether it is still within the visual inference framework remains a question.

Further, this system can only tell us if the input is closer to the null plots or alternative plots trained by the model. It is likely that it can not handle other types of data plots and residual departures that is not similar to the training data. This will be a problem because we do not have training data for various types of residual departures.

The type I error of this system can still be controlled for a new regression model. We can feed plenty of null plots generated by the fitted model and estimate the 5% cutoff from the produced output. This will only be a problem if we do not have enough null plots for the new regression model or the conduction of prediction is time consuming.






  
# Response variable of the CV model

Before we consider the candidate model, we need to consider what we try to model such that a visual test can be conducted by the computer.

One possible and intuitive candidate will be the probability of selecting a plot from a lineup. For the simple Binomial model, this can be  

## $c_i/K$


### Check consistency of $c_i/K$

$$\text{E}[c_i/K] = \sum_{i=1}^{m}\frac{1}{m}p(\text{detect}|s_j = i)p(s_j = i).$$

$c_i/K$ will converge to $\text{E}[c_i/K]$ as $K \to \infty$.

```{r}
vi_survey %>%
  filter(!attention_check) %>%
  arrange(unique_lineup_id, weighted_detect) %>%
  group_by(unique_lineup_id) %>%
  summarise(total_k = n(), 
            ci = cumsum(weighted_detect), 
            k = 1:n(), 
            ci_div_k = cumsum(weighted_detect)/(1:n())) %>%
  ggplot() +
  geom_line(aes(k, ci_div_k, group = unique_lineup_id)) +
  facet_wrap(~total_k, scales = "free_x")
```

Check p-values given by same $c_i/K$ but different $K$.

```{r}
get_p_value <- function(n_eval, average_total_detect, alpha = 0.2) {
  weighted_total_detect <- n_eval * average_total_detect
  target_dist <- exact_dist(n_eval, 20, dist = "dirichlet")
  floor_total <- floor(weighted_total_detect)
  ceil_total <- ceiling(weighted_total_detect)
  return(unname(sum(target_dist[(ceil_total:n_eval) + 1]) + 
                  (ceil_total - weighted_total_detect) * 
                  target_dist[floor_total + 1]))
}

ggplot() +
  geom_point(aes(5:100, map_dbl(5:100, ~get_p_value(.x, 0.2)))) +
  scale_y_log10() +
  xlab("K") +
  ylab("p-value")
```


Given the average weighted detect $c_i/K$, we can not determine the p-value of a visual test. Even the simple Binomial model will suffer from the same issue. 



# Computer vision ideas

Train the model with simulated data and fine-tune it with the human data.

## Inputs and outputs

Input: Single residual plot
Output: The probability of the plot being rejected

Input: Single residual plot
Output: The probability of the plot is not a good residual plot

Raw-Input: Residuals (numerical)
Input: Standard residual plot
Output: The probability of the residuals are not good

Raw-Input: Residuals (numerical)
Input: Standard residual plot
Output: The probability of each departure (multiclass)

Raw-Input: Residuals (numerical)
Input: Standard residual plot & residuals
Output: The probability of the residuals are not good

Input: Multiple residual plots (from a lineup)
Output: Most different plot

Input: Multiple residual plots (from a lineup)
Output: Most different plots

Input: Multiple residual plots (from a lineup)
Output: Ranking (most different plot)

Input: Multiple residual plots (from a lineup)
Output: $c_i/K$ (1, 0.5, ..., 0.05)

## Augmentation

1. Design of the residual plot (cols, line, linetype, etc.)
2. 


## Arch

## Data

## 
